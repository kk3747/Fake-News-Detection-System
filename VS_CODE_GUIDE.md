# Fake News Detection Project - Complete VS Code Guide

## ğŸ¯ Project Overview
This project implements an **Ensemble Graph Neural Network (EGNN)** for fake news detection using NLP and Graph Neural Networks. We've completed **Phase 3** which includes text preprocessing, embedding extraction, and graph construction.

## ğŸ“ Project Structure in VS Code
```
fake_news_project/
â”œâ”€â”€ ğŸ“ dataset/                          # Your real dataset
â”‚   â”œâ”€â”€ gossipcop_fake.csv              # 8,000+ fake news articles
â”‚   â”œâ”€â”€ gossipcop_real.csv              # 8,000+ real news articles
â”‚   â”œâ”€â”€ politifact_fake.csv             # 300+ fake news articles
â”‚   â””â”€â”€ politifact_real.csv             # 300+ real news articles
â”œâ”€â”€ ğŸ“ src/
â”‚   â”œâ”€â”€ ğŸ“ fake_news/                   # Source code
â”‚   â”‚   â”œâ”€â”€ ğŸ“ data/                    # Data processing modules
â”‚   â”‚   â”œâ”€â”€ ğŸ“ features/                # Text preprocessing & embeddings
â”‚   â”‚   â”œâ”€â”€ ğŸ“ graphs/                  # Graph construction & profile features
â”‚   â”‚   â”œâ”€â”€ ğŸ“ models/                  # GNN models (Phase 4)
â”‚   â”‚   â”œâ”€â”€ ğŸ“ training/                # Training pipeline (Phase 5)
â”‚   â”‚   â””â”€â”€ ğŸ“ utils/                   # Utilities
â”‚   â””â”€â”€ ğŸ“ data/                        # ğŸ“Š PROCESSED DATA LOCATION
â”‚       â”œâ”€â”€ ğŸ“ processed/               # Text embeddings & processed data
â”‚       â”‚   â”œâ”€â”€ sample_news_processed.csv    # ğŸ¯ MAIN RESULTS FILE
â”‚       â”‚   â”œâ”€â”€ sample_spacy_embeddings.npy  # spaCy features (300D)
â”‚       â”‚   â”œâ”€â”€ sample_bert_embeddings.npy   # BERT features (768D)
â”‚       â”‚   â”œâ”€â”€ sample_f3_features.npy       # Combined features (315D)
â”‚       â”‚   â””â”€â”€ sample_metadata.json         # Dataset statistics
â”‚       â””â”€â”€ ğŸ“ graphs/                  # ğŸ”— GRAPH STRUCTURES LOCATION
â”‚           â”œâ”€â”€ retweet_networks.pkl         # 50 engagement graphs
â”‚           â”œâ”€â”€ graph_features.pkl           # Graph-level metrics
â”‚           â”œâ”€â”€ node_features.pkl            # Node feature matrices
â”‚           â””â”€â”€ edge_features.pkl            # Edge feature matrices
â”œâ”€â”€ ğŸ“ notebooks/                       # Jupyter notebooks for each phase
â”‚   â”œâ”€â”€ 02_phase2_text_embeddings.ipynb
â”‚   â””â”€â”€ 03_phase3_graph_construction.ipynb
â”œâ”€â”€ ğŸ“„ VS_CODE_GUIDE.md                 # This guide
â”œâ”€â”€ ğŸ“„ DEMO_FOR_PROFESSOR.md            # Professor presentation guide
â”œâ”€â”€ ğŸ“„ show_results.py                  # Results demonstration script
â””â”€â”€ ğŸ“„ requirements.txt                 # Python dependencies
```

## ğŸš€ How to Run the Project in VS Code

### Step 1: Open Project in VS Code
1. Open VS Code
2. File â†’ Open Folder
3. Select: `/Users/hemantkumar/Desktop/fake_news_project`

### Step 2: Set Up Python Environment
1. Open VS Code Terminal (Ctrl+` or View â†’ Terminal)
2. Create virtual environment:
   ```bash
   python3 -m venv .venv
   source .venv/bin/activate
   ```
3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   python3 -m spacy download en_core_web_sm
   ```

### Step 3: Run Phase 3 Processing
In VS Code Terminal, run:
```bash
# Set Python path and run processing
export PYTHONPATH=/Users/hemantkumar/Desktop/fake_news_project/src
python3 src/fake_news/graphs/process_sample.py
```

**Expected Output:**
```
2025-10-06 04:19:02,975 | INFO | fake_news.__main__ | Processing sample dataset with 50 articles...
2025-10-06 04:19:03,667 | INFO | fake_news.__main__ | Sample dataset: 50 articles
2025-10-06 04:19:03,668 | INFO | fake_news.__main__ | Fake: 25, Real: 25
...
Sample processing completed successfully!
Processed 50 articles
F3 features shape: (50, 111)
Created 50 engagement graphs
```

### Step 4: View Results
Run the demonstration script:
```bash
python3 show_results.py
```

## ğŸ“Š Where to Find Results in VS Code

### ğŸ¯ Main Results File
**Location**: `src/data/processed/sample_news_processed.csv`
- **How to open**: Click on the file in VS Code Explorer
- **Content**: 50 processed news articles with all features
- **Columns**: id, title, label, title_processed, profile_* features

### ğŸ”— Graph Structures
**Location**: `src/data/graphs/retweet_networks.pkl`
- **How to view**: Use Python script (see below)
- **Content**: 50 NetworkX graphs representing user engagement

### ğŸ“ˆ Feature Files
**Location**: `src/data/processed/`
- `sample_spacy_embeddings.npy` - spaCy features (50Ã—300)
- `sample_bert_embeddings.npy` - BERT features (50Ã—768)
- `sample_f3_features.npy` - Combined features (50Ã—315)

## ğŸ” How to View Different Results

### 1. View Processed News Data
Create a new Python file in VS Code and run:
```python
import pandas as pd
import sys
sys.path.append('src')

# Load processed data
df = pd.read_csv('src/data/processed/sample_news_processed.csv')

# View first 5 rows
print("ğŸ“Š PROCESSED NEWS DATA:")
print(df[['id', 'title', 'label', 'title_processed']].head())

# View dataset summary
print(f"\nğŸ“ˆ DATASET SUMMARY:")
print(f"Total articles: {len(df)}")
print(f"Fake news: {df['label'].sum()}")
print(f"Real news: {len(df) - df['label'].sum()}")
```

### 2. View Graph Structures
Create a new Python file and run:
```python
import pickle
import networkx as nx
import sys
sys.path.append('src')

# Load graphs
with open('src/data/graphs/retweet_networks.pkl', 'rb') as f:
    graphs = pickle.load(f)

# Show graph information
print("ğŸ”— ENGAGEMENT GRAPHS:")
print(f"Total graphs: {len(graphs)}")

# Show sample graph
sample_id = list(graphs.keys())[0]
sample_graph = graphs[sample_id]
print(f"\nSample graph ({sample_id}):")
print(f"Nodes: {sample_graph.number_of_nodes()}")
print(f"Edges: {sample_graph.number_of_edges()}")

# Show node types
node_types = nx.get_node_attributes(sample_graph, 'node_type')
news_nodes = sum(1 for nt in node_types.values() if nt == 'news')
user_nodes = sum(1 for nt in node_types.values() if nt == 'user')
print(f"News nodes: {news_nodes}")
print(f"User nodes: {user_nodes}")
```

### 3. View Embeddings
Create a new Python file and run:
```python
import numpy as np
import sys
sys.path.append('src')

# Load embeddings
spacy_emb = np.load('src/data/processed/sample_spacy_embeddings.npy')
bert_emb = np.load('src/data/processed/sample_bert_embeddings.npy')
f3_features = np.load('src/data/processed/sample_f3_features.npy')

print("ğŸ”¤ TEXT EMBEDDINGS:")
print(f"spaCy shape: {spacy_emb.shape}")
print(f"BERT shape: {bert_emb.shape}")
print(f"F3 shape: {f3_features.shape}")

print(f"\nspaCy range: [{spacy_emb.min():.3f}, {spacy_emb.max():.3f}]")
print(f"BERT range: [{bert_emb.min():.3f}, {bert_emb.max():.3f}]")
print(f"F3 range: [{f3_features.min():.3f}, {f3_features.max():.3f}]")
```

## ğŸ“‹ Quick Commands for VS Code Terminal

### Run Complete Demo
```bash
# Set Python path
export PYTHONPATH=/Users/hemantkumar/Desktop/fake_news_project/src

# Run processing
python3 src/fake_news/graphs/process_sample.py

# Show results
python3 show_results.py
```

### Check File Sizes
```bash
ls -la src/data/processed/
ls -la src/data/graphs/
```

### View CSV Content
```bash
head -5 src/data/processed/sample_news_processed.csv
```

## ğŸ“ What Each Phase Does

### âœ… Phase 1: Project Setup
- Created project structure
- Set up data directories
- Created utility modules

### âœ… Phase 2: Text Preprocessing + Embeddings
- Text cleaning and preprocessing
- spaCy embeddings (300D)
- BERT embeddings (768D)

### âœ… Phase 3: Graph Construction + Profile Features
- User profile feature extraction (15D)
- Engagement graph construction
- F3 features (spaCy + Profile = 315D)

### ğŸ”„ Phase 4: GNN Models (Next)
- GCN, GAT, BiGCN models
- Training pipeline
- Model evaluation

### ğŸ”„ Phase 5: Ensemble Methods (Next)
- Majority voting
- Weighted average
- Stacking ensemble

### ğŸ”„ Phase 6: Evaluation (Next)
- Focal loss implementation
- Performance metrics
- Visualizations

## ğŸ“Š Results Summary

### Dataset Processed
- **50 articles** from GossipCop dataset
- **25 fake news**, **25 real news**
- **Text preprocessing**: Lowercase, tokenization, stemming
- **Feature extraction**: spaCy (300D), BERT (768D), Profile (15D)
- **Graph construction**: 50 engagement networks

### Key Results Files
1. **`sample_news_processed.csv`** - Main processed data (125KB)
2. **`retweet_networks.pkl`** - 50 engagement graphs (23KB)
3. **`sample_spacy_embeddings.npy`** - spaCy features (19KB)
4. **`sample_bert_embeddings.npy`** - BERT features (154KB)
5. **`sample_f3_features.npy`** - Combined features (45KB)

## ğŸ”§ Troubleshooting

### If you get import errors:
```bash
export PYTHONPATH=/Users/hemantkumar/Desktop/fake_news_project/src
```

### If you get module not found errors:
```bash
pip install pandas numpy nltk scikit-learn matplotlib seaborn tqdm spacy transformers torch
python3 -m spacy download en_core_web_sm
```

### If you want to process more data:
Edit `src/fake_news/graphs/process_sample.py` and change:
```python
result = process_sample_dataset(100)  # Process 100 articles instead of 50
```

## ğŸ“ Support
- All source code is in `src/fake_news/` directory
- Check `DEMO_FOR_PROFESSOR.md` for professor presentation
- Run `python3 show_results.py` for complete results overview

## âœ… Ready for Phase 4
All data is processed and ready for GNN model training!
The results are located in `src/data/processed/` and `src/data/graphs/` directories.
