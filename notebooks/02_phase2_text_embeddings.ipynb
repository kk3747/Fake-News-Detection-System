{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 2: Text Preprocessing + Embeddings\n",
        "\n",
        "This notebook demonstrates the text preprocessing and embedding extraction pipeline as described in the research paper.\n",
        "\n",
        "## What we'll do:\n",
        "1. **Text Preprocessing**: Clean, tokenize, remove stopwords, apply stemming\n",
        "2. **spaCy Embeddings**: 300-dimensional word2vec-based features\n",
        "3. **BERT Embeddings**: 768-dimensional contextual features\n",
        "4. **Save/Load**: Cache embeddings for reuse\n",
        "\n",
        "## Paper Reference:\n",
        "- Text preprocessing follows the methodology in Section 6.2.1\n",
        "- spaCy features: 300-dimensional word2vec vectors\n",
        "- BERT features: 768-dimensional contextual embeddings\n",
        "- Both are used as F1 and F2 feature sets in the paper\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append('../src')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "from fake_news.features.text_preprocessing import TextPreprocessor, preprocess_news_data\n",
        "from fake_news.features.embeddings import EmbeddingExtractor, create_synthetic_data\n",
        "from fake_news.utils.logging import get_logger\n",
        "from fake_news.utils.paths import PROCESSED_DIR\n",
        "\n",
        "logger = get_logger('notebook')\n",
        "logger.info('Phase 2: Text Preprocessing + Embeddings')\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
